---
title: "A new approach to gray whale abundance estimation"
author: "Tomo Eguchi"
date: "`r Sys.Date()`"
output: 
  bookdown::word_document2: 
    reference_docx: reference.docx
bibliography: reference.bib
csl: marine-ecology-progress-series.csl
---

```{r setup, include=FALSE}
#  bookdown::word_document2: default
#  number_sections: true
#  reference_docx: style_reference.docx
#    base_format: "bookdown::word_document2"


rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      cache = TRUE)
save.fig <- T

source("Granite_Canyon_Counts_fcns.R")
library(tidyverse)
library(lubridate)
library(flextable)
library(jagsUI)
library(bayesplot)
library(ggpubr)
#library(R2WinBUGS)
library(abind)
library(rmarkdown)
library(loo)
library(ggridges)
library(rstanarm)
library(mgcv)
library(flextable)
library(officer)
library(officedown)

options(mc.cores = 5)

set_flextable_defaults(font.size = 12,
                       font.family = "Aptos")

# Unicode for Rhat:
R_hat <- "R\u0302"

YEAR <- 2026
min.dur <- 60#10 #
dpi.set <- 600

# This run (2025-09-19) includes posterior samples of obs.prob. It was run in
# the GrayWhaleAbundance project. 
# The run of 2025-04-11 is the one for the Tech Memo
WinBUGS.Run.Date <- "2025-09-19"  

save.fig.fcn <- function(fig, file.name, replace = F, 
                         dpi = dpi.set, device = "png",
                         height = 3, width = 3, units = "in",
                         bg = "white"){
  if (isTRUE(replace) | (!isTRUE(replace) & !file.exists(file.name)))
    ggsave(fig, 
           filename = file.name, 
           dpi = dpi, device = device,
           height = height, width = width, units = units,
           bg = bg)
  
}


```

## Introduction {.unnumbered}

The Eastern North Pacific (ENP) gray whale (*Eschrichtius robustus*) population undertakes one of the longest annual migrations of any mammal, traveling between its Arctic feeding grounds and the warm-water lagoons of Baja California, Mexico, where they calve and breed. This iconic population has been the subject of extensive research due to its remarkable recovery from historic whaling and its continued vulnerability to environmental changes and anthropogenic impacts. Understanding the dynamics of the ENP gray whale population, including its abundance, distribution, and migratory patterns, is crucial for effective conservation and management.

Analytical methods to estimate the abundance of gray whales from visual surveys at Granite Canyon, CA, have evolved over the years [@laake_gray_2012; @durban_estimating_2015]. Laake et al. (2012) used the distance sampling approach with generalized additive models (GAMs). Durban et al. (2015) developed a new method using a Bayesian N-mixture approach, which was approved by the International Whaling Commission (IWC), and it has been used for the analysis since the 2015/2016 season. In their approach, Durban et al. used the observed changes in migrating gray whales at a single location along the migration corridor. The general trend of migration is that the number of gray whale sightings increases over time, reaches a peak, and then decreases. Durban et al.’s method used the Gaussian function to capture this general trend, while deviations from the Gaussian function were modeled via fitting a spline function to the observed counts. Observations, then, were modeled with binomial distributions (i.e., N-mixture model). The approach required double-observer protocols so that detection probabilities (i.e., probabilities in the binomial distributions) can be estimated. In recent years, however, financial and logistical constraints prohibited from having two independent simultaneous observation teams. Furthermore, the analysis was conducted using WinBUGS, which had become largely obsolete over the past decade.

This report presents improvements to the method developed by Durban et al. by replacing the binomial sampling model with the Poisson sampling model so that data from single observer team can be used to estimate abundance. I  developed this analysis using JAGS, which is another MCMC sampler but more contemporary. I will first briefly describe the method by Durban et al. and identify its underlying assumptions. Then, I will introduce a new approach that is consistent with the foundational concepts of the Durban et al. method but improves upon it. Finally, I reanalyze the data from Granite Canyon, CA, to compare abundance estimates among the three approaches: Laake et al. (2012), Durban et al. (2015), and the new approach presented here.

### Method by Durban et al. (2015) {.unnumbered}

To estimate the detectability of gray whales by visual observers at the field station located at Granite Canyon, CA, counts from two independent stations of paired observers operating simultaneously were compared during two seasons (2009/2010 and 2010/2011). The two watch stations were positioned 35 m apart at the same elevation (22.5 m) above sea level (Durban et al. 2015). For the years with only one station, the detectability was extrapolated for all monitored watch periods based on the fitted model for the detectability, where the counts for the south watch station were treated as zero inflated binomial outcomes. The binomial probability was specified as the product of an indicator function and the detectability ($u_{i,j,t} \times p_{i,j,t}$), where $u$ = 1 or 0 to indicate whether or not count data were collected from that station, respectively. This formulation ensured that structural zero counts from periods without a second watch did not contribute to the likelihood for estimation of $p$ or $N$ (Durban et al. 2015).

Consistent with Laake et al. (2012), the model for the detection probability incorporated fixed effects for visibility ($VS$) and the Beaufort sea state ($BF$), whereas observers were treated as random effects ($O$). These were modeled as additive effects on a general intercept so that the direction and magnitude of the estimated effects away from zero (no effect) could be assessed. The selection for the inclusion of these effects was accomplished by using Bayesian model selection with stochastic binary indicator variables $I$ to switch each of the three possible effects (i.e., $V$, $B$, and $O$) either in or out of the model:

\begin{equation}
logit(p_{i,d(j),t}) = logit(p_0) + I_{V} \beta_{V} V_{d(j),t} + I_{B} \beta_{B} B_{d(j),t}+ I_{O} \mathbf{\beta}^{O_{i,d(j),t} = o}_{O} (\#eq:detection-probability-Durban)
\end{equation}

where the intercept $p_0$ was the base detection probability in the absence of covariate effects, assigned a Uniform(0,1) prior distribution, and $logit(p_0) = ln(p_0/(1–p_0))$. For each fixed effect, $\beta_{B}$ and $\beta_{V}$, a Gaussian prior distribution with mean zero and standard deviation of 10 was used. The random effect for each observer was drawn from a Gaussian distribution with mean zero and standard deviation $\sigma_{O} \sim Uniform(0,10)$. Each binary indicator variable, $I$, was assigned a Bernoulli(0.5) distribution to specify equal probability of inclusion (1) or exclusion (0) of the effect in the model (Durban et al. 2015).

Observation probability, then, was computed by multiplying the detection probability with an indicator function for whether or not an observer was present ($u$):

$$ obs.prob = u \times p $$

Coefficients were computed separately for the Gaussian and spline functions, where one or the other was selected based on which function was deemed appropriate for each day.

A mathematical description of the approach by Durban et al. is provided in the Appendix.

### Limitations of the Durban et al method {.unnumbered}

Durban et al. (2015) used the "cut" function within WinBUGS to dissociate estimated parameters between the "Common" ($f_{d,t}$) and "Specific" ($g_{d,t}$) models. The function (*cut*) is unavailable in modern Bayesian computation packages (e.g., JAGS, STAN) and its use has been questioned [@plummer_cuts_2015]. In short, the "cut" function does not converge to a well-defined limiting distribution (Plummer 2015). Furthermore, the assumption that the number of migrating gray whales in front of an observation station follows a Gaussian distribution centered around the midpoint of the season is a potential limitation. The actual migration curve may not be symmetric, and the peak abundance may persist for several days rather than being instantaneous. While fitting spline functions to observed counts may alleviate some of these problems, a fitted spline can be overly flexible. A spline fit can fail to preserve the general migration assumption that the number of whales increases from the beginning of a migration season, reaches a peak, and then decreases toward the end of the season. Additionally, the Durban et al. method implicitly assumed that two observation stations operated simultaneously with identical effort. In reality, however, watch durations were not always identical between the two stations in previous years. Lastly, the N-mixture approach performs better if there were at least two independent counts of the same process to estimate detection probabilities. Financial and logistical constraints have made using two independent observers impossible. Consequently, the model has to be changed to accommodate this limitation. 

In their analysis, convergence of the WinBUGS analysis was not thoroughly determined. Since the study was published in 2015, new tools were developed to more comprehensively evaluate convergence of MCMC samples (e.g., rank-normalized Rhat statistic and bulk and tail effective sample sizes; Vehtari et al. 2020?). I will use these tools to compare the two Bayesian approaches. 

## Methods {.unnumbered}

### Data {.unnumbered}

A series of modifications to data collection methods was implemented from the 1967/1968 to 2006/2007 seasons (Laake et al. 2012). Starting in the 2010/2011 season, further modifications were implemented, using a computer to predict movements of gray whale groups, thereby increasing the likelihood of repeatedly observing the same groups and more precicely  determining the number of individuals in each group. In Durban et al. (2015), they treated the data from previous years as incompatible in their analysis. Consequently, the subsequent analysis to estimate gray whale abundance used the data since the 2006/2007 season (e.g., Stewart and Weller 2021, Eguchi et al., 2022, 2023, 2024, 2025). Since the 2010/2011 season, no dual-observer survey has been conducted to estimate detection due to the logistical and financial constraints. Consequently, the data from the two seasons (i.e., 2009/2010 and 2010/2011 seasons) are the primary source of information for estimating detection probabilities. During several years before the 2006/2007 season, two observation stations were present. In those years, sightings of the same groups by two teams were used to estimate detection probabilities via the "capture-recapture" method. These sightings data can be used by retaining the number of observed groups but disregarding the group identity. In this paper, I developed a new model to accommodate the constraints on data collection, while allowing to use all the data from the 1967/1968 season by using flexible statistical models.

### New Modeling Approach {.unnumbered}

A primary limitation of the Durban et al. approach was the difficulty of handling missing data points (e.g., weekends, seasons with fewer than 90 observation days, and short survey shifts due to inclement conditions). To address this and other limitations, I propose a new approach that replaces the Gaussian distribution and spline functions with a single flexible function.

This new approach uses a function that can accommodate an asymmetrical shape around the peak, while reflecting the general pattern of increase, peak, and decrease in whale counts. This pattern can be modeled with the product of two sigmoid equations or mathematical functions that have a peak that is not necessarily centered. A similar pattern is often found in the number of nesting marine turtles. For instance, a model derived from a modified Verhulst equation was used to describe the number of nesting leatherback turtles in French Guiana and Suriname [@girondot_modeling_2007]. The equation is similar to the Richards growth function [@richards1959] and has the following form.

$$M_1 = (1 + (2 e^K - 1) * e^{(P_1-d)/(-S_1)}) ^ {(-1/e^K)}$$

$$M_2 = (1 + (2 e^K - 1) * e^{(P_2-d)/S_2}) ^ {(-1/e^K)}$$

$$\bar{N} = N_{min} + (N_{max} - N_{min}) * (M_1 * M_2)$$

where $d > 0$ is the number of days since an arbitrary start date, $S_1 > 0$ and $S_2 > 0$ define how the slope decreases and increases, respectively, $K > 0$ defines the "flatness" at the peak of the function, $P_1$ and $P_2$ define where the peak is relative to the range of $d$, where $min(d) < P_1 <= P_2 < max(d)$, $N_{min}$ is the number of whales migrating outside of a migration season ($N_{min} = 0$), and $N_{max} >> N_{min}$. $N_{max}$ is not the maximum number of whales migrating per day, but it is a parameter that may be fixed or estimated during the analysis. Effects of these parameters on the shape of the equation are described in the Appendix. Hereafter, this function will be referred to as the Richards equation.

I created eight versions of the Richards equation, based on how $P_1$, $P_2$, $S_1$, and $S_2$ were defined with respect to time (Table \@ref(tab:model-definition)). The $N_{max}$ parameter was assumed to be season-specific and $K$ was assumed to be constant (0.01) across all seasons. The parameters $P$, or $P1$ and $P2$, and $Max$ were modeled as autoregressive (AR(1)). For example, 

$$ \bar{P}[y] = \mu_P + \rho_P \times (P[y-1] - \mu_P)$$

$$ P[y] \sim N(\bar{P}[y], \sigma_P) $$

$$ P[1] \sim N(\mu_P, \sigma_P) $$

For the Max parameter, the autoregressive process was modeled in the natural log space:

$$ log(\bar{Max}[y]) = \mu_{log(Max)} + \rho_{Max} \times (log(Max[y-1]) - \mu_{log(Max)} $$
        
$$ log(Max[y]) \sim N(log(\bar{Max}[y], \sigma_{Max}) $$ 

$$ Max[y] = exp(log(Max[y]) $$ 

Each parameter had a hyper-distribution of either a Gaussian, uniform, or gamma distribution. Uniform distributions were used for the prior distributions of hyperparameters (Appendix Table A`r run_reference("hyperparameter-table")`).

```{r model-definitions-table, echo=FALSE, message=FALSE}
model.defs <- data.frame(ID = c(1:8),
                         P1 = c(rep("Season", 4), rep("P[Season]", 4)),
                         P2 = c(rep("Season", 4), rep("P[Season]", 4)),
                         S1 = rep(c("Season", "Constant"), 4),
                         S2 = rep(c("Season", "Constant", "Constant", "Season"), 2))

ft.models <- flextable(model.defs)
header_labels <- list(
  ID = as_paragraph(as_chunk("Model")),
  P1 = as_paragraph(as_chunk("P"), as_chunk("1", props = fp_text(vertical.align = "subscript"))),
  P2 = as_paragraph(as_chunk("P"), as_chunk("2", props = fp_text(vertical.align = "subscript"))),
  S1 = as_paragraph(as_chunk("S"), as_chunk("1", props = fp_text(vertical.align = "subscript"))),
  S2 = as_paragraph(as_chunk("S"), as_chunk("2", props = fp_text(vertical.align = "subscript")))
)

for (nm in names(header_labels)) {
  ft.models <- flextable::compose(ft.models, 
                                  part = "header", 
                                  j = nm, 
                                  value = header_labels[[nm]])
}

```


```{r table-model-definition, echo=F, message=F}

ft.models %>%
  # 1. Use Word's native autofit instead of R's autofit()
  # This prevents the calculation error that causes the crash
  set_table_properties(layout = "autofit") %>%
  
  # 2. Set pagination
  paginate(init = TRUE, hdr_ftr = TRUE) %>% 
  
  # 3. Add caption last
  set_caption(
    caption = as_paragraph(
      "Definitions of eight models. Four parameters (P", as_sub("1"), 
      ", P", as_sub("2"), ", S", as_sub("1"), ", S", as_sub("2"), 
      ") were tested for time dependence. P", as_sub("1"), 
      " = P", as_sub("2"), " = P[Season] means that there was one P for each season."),
    fp_p = officer::fp_par(text.align = "left"),
    align_with_table = FALSE,
    autonum = run_autonum(
      seq_id = "tab", 
      bkm = "model-definition"
    ),
    style = "Table Caption"
  )

# ft.models %>%
#   autofit() %>%
#   paginate(init = TRUE, hdr_ftr = TRUE) %>%
#   set_caption(caption = as_paragraph("Definitions of eight models. Four parameters (P", as_sub("1"), ", P", as_sub("2"), ", S", as_sub("1"), ", S", as_sub("2"), ") were tested for time dependence. P", as_sub("1"), " = P", as_sub("2"), " = P[Year] means that there was one P for each year."),
#               autonum = run_autonum(seq_id = "tab",
#                                     bkm = "model-definitions"),
#               style = "Table Caption") 

```

The Richards equation provided the daily mean number of gray whales ($\bar{N}$) passing through the observation point. The number of available whales for sampling was determined by considering the shift duration in days:

$$ A_{t,s,y} = \bar{N} \times L(i_t,s,y)$$

The observed number of whales is a Poisson deviate with the mean ($\lambda$), where $\kappa$ is a function of $\A_{t,s,y}$ and the sighting probability ($p_{i_t, s, y}$):

$$ n_{i_t, s, y} \sim POI(\kappa_{i_t, s, y})  $$

where the index $i_t$ is the watch period $i$ of the day $t$, $s$ is the station (1 or 2), and $y$ is the season.

\begin{equation} logit(p_{i_t, s, y}) = \alpha[O_{i_t, s, y}] + \beta_{B} \times B_{i_t, y} + \beta_{V} \times V_{i_t, y} 
(\#eq:detection-probability)
\end{equation}


where the observation probability ($p_{i_t, s, y}$) was a function of the viewing conditions (the Beaufort sea state ($B$) and visibility code ($V$)), the overall mean probability of detection ($\beta_0$), and observer effects ($O$). For the observer effects ($\alpha[O_{i_t, s, y}]$), individuals with 10 most sightings were selected, whereas the rest of observers were pooled as one observer. 

The total number of gray whales for the season $y$ is the sum of all $\bar{N}_{t, y}$ and corrected for nighttime passage:

$$ N_y = \lambda * \sum_{t = 1} ^ {max(d)} N_{t, y} $$ and

$$ \lambda \sim N(1.0875, 0.03625) $$ (Perryman et al. 1999).

```{r hyper-parameters, echo=FALSE, message=FALSE}

hyper.params <- data.frame(Parameter = c("P[1]", "mu.P", "sigma.P", "rho.P", "log(Max[1])", "mu.log.Max", "sigma.log.Max", "beta.BF", "beta.VS", "S.alpha", "S.beta", "alpha"),
                           Distribution = c("Normal(mu.P, sigma.P)",
                                            "Uniform(30, 60)",
                                            "Normal(0, 1)T(0,)",
                                            "Uniform(-1, 1)",
                                            "Norm(mu.log.Max, sigma.log.Max)", 
                                            "Normal(7.6, 4)",
                                            "Normal(0, 1)T(0,)",
                                            "Normal(0, 4)",
                                            "Normal(0, 4)",
                                            "Normal(10, 10)T(0,)",
                                            "Gamma(1, 1)",
                                            "Normal(0, 1)"))

hyper.params %>%
  flextable() %>%
  set_table_properties(layout = "autofit") %>%
  #autofit() %>%
  paginate(init = TRUE, hdr_ftr = TRUE) %>%
  compose(i = 1, j = "Parameter",
          value = as_paragraph(as_chunk("N"), 
                               as_chunk("Max", props = fp_text(vertical.align = "subscript")))) %>%
  compose(i = 2, j = "Parameter",
          value = as_paragraph(as_chunk("S"), 
                               as_chunk("1", props = fp_text(vertical.align = "subscript")))) %>%
  compose(i = 3, j = "Parameter",
          value = as_paragraph(as_chunk("S"), 
                               as_chunk("2", props = fp_text(vertical.align = "subscript")))) %>%
  compose(i = 7, j = "Parameter",
          value = as_paragraph(as_chunk("\U03B2"), 
                               as_chunk("B", props = fp_text(vertical.align = "subscript")))) %>%
  compose(i = 8, j = "Parameter",
          value = as_paragraph(as_chunk("\U03B2"), 
                               as_chunk("V", props = fp_text(vertical.align = "subscript")))) %>%
  compose(i = 9, j = "Parameter",
          value = as_paragraph(as_chunk("\U03B2"), 
                               as_chunk("0", props = fp_text(vertical.align = "subscript"))))  -> ft.hyper.params
  

```

In the Durban et al. analysis, the annual migration at the survey station was defined as a 90-day period starting on December 1, with the number of passing whales assumed to be zero on days 1 and 90 (February 29th or March 1st). However, in some years, observations continued beyond March 1st. To incorporate all available data, I defined the annual migration period as 100 days (ending on March 10th or 11th), assuming the number of passing whales was zero on days 1 and 100 ($max(d) = 100$).

The new approach was used to analyze the entire dataset from the 1967/1968 season onward, and the results were compared to the estimates reported in Eguchi et al. (2025). In order to apply the new approach to historical data, sighting data between the 1967/1968 and 2006/2007 seasons were obtained from the *ERAnalysis* package (<https://github.com/jlaake/ERAnalysis>, Laake 2012). An R script was written to convert the historical sightings data in the package to an input object required for the new analysis. Data from the 2007/2008 season to the 2024/2025 season were extracted from raw data files using another R script. All shifts of at least 60 minutes with the Beaufort sea state of \< 5 and the visibility code of \< 5 were included in the analysis. Laake et al.’s approach was applied to the entire dataset after formatting new data from the 2007/2008 season into the input object. Because no information was available to match sightings of groups between the two stations in the 2009/2010 and 2010/2011 seasons, a correction factor, i.e., a multiplier to convert “naïve” to “corrected,” which accounted for missed groups by the primary observer, from double-observer years, was applied. 

The new models were fitted to the data using JAGS (Plummer) in the R statistical environment, using the *jagsUI* package (Kellner 2024). For each model, five MCMC chains were run for 250,000 steps each. The first 50,000 samples of each chain were discarded as “burn-in.” The remaining posterior samples were thinned by taking every 200 samples, resulting in 5000 posterior samples for inference. The convergence of MCMC was assessed using the rank-normalized `r R_hat` statistic and effective sample sizes (ESS) [@vehtari2021]. For the models with sufficient convergence, goodness-of-fit was determined via Pareto-k statistics [@vehtari_practical_2017] implemented in the *loo* package (Vehtari et al. 2024). The performance of models was compared using the LOOIC (Leave-One-Out Information Criteria) statistic via the *loo* and *rstanarm* (Goodrich et al. 2024) packages. 

To compare the three approaches (Laake et al., Durban et al., and the new approach), I fitted generalized additive models (GAMs) to abundance estimates between the 2006/2007 and 2024/2025 seasons. Laake et al.'s and the new approaches were compared by fitting GAMs to the point estimates over the entire timeseries from the 1967/1968 to 2024/2025 seasons. The rate of recent decline in the estimated abundance was compared by fitting linear models to the abundance estimates between the 2015/2016 and 2024/2025 seasons.

JAGS models, R scripts to conduct the analysis, including data extraction and executing the JAGS models, and data are available at the GitHub repository (TO BE CREATED).

## Results {.unnumbered}

```{r model-comparison, echo=FALSE, message=FALSE, warning=FALSE}
# Model comparison was completed in Jags_Richards_ModelComparison.R and
# results saved in a .rds file. To change that, run the script again.
# 
out.table <- readRDS(paste0("RData/Richards_ModelComparison_", YEAR, ".rds"))
out.table %>% 
  flextable() %>%
  colformat_double(j = c("dLOOIC", "p.big.Rhat", "p.bad.Pareto",
                         "min.ESS.bulk", "min.ESS.tail"),
                   digits = 2) %>%
  compose(part = "header", j = "dLOOIC",
          value = as_paragraph(as_chunk("\u0394"), as_chunk("LOOIC"))) -> ft.models.comp

# ft.models.comp <- flextable(out.table)
# 
# ft.models.comp <- flextable::colformat_double(ft.models.comp,
#                                            j = c("dLOOIC", "p.big.Rhat", "p.bad.Pareto"),
#                                            digits = 2)
# 
# ft.models.comp <- flextable::compose(ft.models.comp, part = "header", j = "dLOOIC",
#                           value = as_paragraph(as_chunk("\u0394"), as_chunk("LOOIC")))

# Fix the column headers:

header_labels <- list(
  model = as_paragraph(as_chunk("Model")),
  n.params = as_paragraph(as_chunk("# parameters")),
  n.big.Rhat = as_paragraph(as_chunk(paste0("# ", R_hat, " > 1.01"))),
  p.big.Rhat = as_paragraph(as_chunk(paste0("% ", R_hat, " > 1.01"))), 
  n.bad.Pareto = as_paragraph(as_chunk("# Pareto-k > 0.7")),
  p.bad.Pareto = as_paragraph(as_chunk("% Pareto-k > 0.7")),
  min.ESS.bulk = as_paragraph(as_chunk("Minimum bulk ESS")),
  min.ESS.tail = as_paragraph(as_chunk("Minimum tail ESS")))

for (nm in names(header_labels)) {
  ft.models.comp <- flextable::compose(ft.models.comp, 
                                       part = "header", 
                                       j = nm, 
                                       value = header_labels[[nm]])
}

#ft.models.comp <- autofit(ft.models.comp)
```

The eight models (Table \@ref(tab:model-definition)) were fitted to the sightings data over 32 seasons. Three models (`r out.table %>% filter(min.ESS.bulk > 400) %>% pull(model)`) indicated satisfactory convergence of MCMC based on the minimum bulk ESS values (Table \@ref(tab:model-table)), whereas the rest did not. Among the three models, based on LOOIC, Model `r` was identified as the best-fitting model (Table \@ref(tab:model-table)). The model consists of year-specific $P$ ($P_1 = P_2$), $S_1$, and $S_2$ parameters (Table \@ref(tab:model-definition)).

```{r table-models, echo=F, message=F}
ft.models.comp %>%
  set_table_properties(layout = "autofit") %>%
  #autofit() %>%
  paginate(init = TRUE, hdr_ftr = TRUE) %>%
  set_caption(caption = as_paragraph("Comparison of eight models using the LOOIC, rank-normalized ", as_chunk(R_hat), " statistics, Pareto-k statistics, and effective sample sizes (ESS bulk and tail). LOOIC (leave-one-out information criteria) measures the predictive accuracy of each model, where smaller values indicate better predictions. ", as_chunk("\u0394"), as_chunk("LOOIC"), " is the difference between a LOOIC value and the minimum LOOIC value. “# parameters” means the number of monitored parameters and not the total number of parameters in the model.  The rank-normalized ", as_chunk(R_hat), " statistic measures the convergence of Markov chain Monte Carlo samples. ", as_chunk(R_hat), " > 1.01 indicates non-convergence. The Pareto-k statistic provides a measure of predictive errors for each data point. The ", as_chunk(R_hat), " statistic is computed for each parameter, and Pareto-k statistic is computed for each data point. ESS is a measure of numerical convergence of Markov chain Monte Carlo. Higher numbers indicate better convergence."),
              fp_p = officer::fp_par(text.align = "left"),
              align_with_table = FALSE,
              autonum = run_autonum(seq_id = "tab",
                                    bkm = "model-table"),
              style = "Table Caption") 
```

```{r JAGS-results, echo=F,message=FALSE}
#rm(list = c(".out"))
ver <- 7 #5 #out.table$model[1] #5
jm.out <- readRDS(paste0("RData/JAGS_Richards_HSSM_v",
                         ver,
                         "a1_1968to2025_min", min.dur,
                         "_NoBUGS.rds"))

# LOOIC and Pareto-k stats
LOOIC.n <- compute.LOOIC(loglik.array = jm.out$jm$sims.list$log.lkhd,
                         MCMC.params = jm.out$MCMC.params)

# better ESS computations:
summary.posterior <- jm.out$posterior.summary

ESS.bulk <- summary.posterior %>%
  select(variable, ess_bulk) %>%
  na.omit() %>%
  arrange(ess_bulk)

ESS.tail <- summary.posterior %>%
  select(variable, ess_tail) %>%
  na.omit() %>%
  arrange(ess_tail)

# Match Pareto-k and data. Note that days 1 and 100 should be removed.
n.array <- jm.out$jags.input$jags.data$n
days <- jm.out$jags.input$jags.data$day
watch.length <- jm.out$jags.input$jags.data$watch.length

# Create an array with corresponding years in non-NA elements
year.array <- array(dim = dim(n.array))
all.start.years <- c(jm.out$jags.input$jags.input.Laake$all.start.year,
                     jm.out$jags.input$jags.input.new$start.years)

for (k in 1:length(all.start.years)){
    year.array[, , k] <- all.start.years[k]
}

n.vec <- n.array[days > 1 & days < 100] %>% na.omit()
day.vec <- days[days > 1 & days < 100] %>% na.omit()
year.vec <- year.array[days > 1 & days < 100] %>% na.omit()

# watch.length does not have day 1 and day 100.
# To use the same selection using days, add zero rows:
zero.rows <- array(data = 0, dim = c(1, 2, dim(n.array)[3]))
watch.length <- abind(zero.rows, watch.length, along = 1)
watch.length <- abind(watch.length, zero.rows, along = 1)

watch.length.vec <- watch.length[days > 1 & days < 100]  %>% na.omit()

Pareto.df <- data.frame(start.year = year.vec,
                        season = paste0(year.vec, "/", (year.vec+1)),
                        day = day.vec,
                        n = n.vec,
                        watch.length = watch.length.vec,
                        Pareto.k = LOOIC.n$loo.out$diagnostics$pareto_k) %>%
  mutate(Pareto.group = ifelse(Pareto.k < 0.7, "good", "bad"),
         N.hat = n/watch.length)


p.Pareto.k <- ggplot(Pareto.df ) +
  geom_point(aes(x = day, y = n, color = Pareto.group)) +
  facet_wrap(~season)

all.start.year <- c(jm.out$jags.input$jags.input.Laake$all.start.year,
                    jm.out$jags.input$jags.input.new$start.years)

# Create a dataframe with all years, including unsampled years.
all.years <- data.frame(start.year = seq(min(all.start.year), max(all.start.year))) %>%
  mutate(Season = paste0(start.year, "/", start.year + 1))

# Look at the annual abundance estimates:
Nhat. <- data.frame(Season = paste0(all.start.year, "/",
                                    all.start.year+1),
                    Nhat = jm.out$jm$q50$Corrected.Est,
                    LCL = jm.out$jm$q2.5$Corrected.Est,
                    UCL = jm.out$jm$q97.5$Corrected.Est) %>%
  right_join(all.years, by = "Season") %>%
  arrange(start.year) %>%
  mutate(Method = paste0("Eguchi M", ver))

# Create an observed counts dataframe
obs.day <- jm.out$jags.input$jags.data$day[,1,]
obs.n <- jm.out$jags.input$jags.data$n[,1,]
obs.prop <- jm.out$jags.input$jags.data$watch.length[,1,]
obs.prop <- rbind(c(rep(0, ncol(obs.n))),
                  obs.prop,
                  c(rep(0, ncol(obs.n))))

obs.n.df <- data.frame(start.year = rep(all.start.year, each = 100),
                       Season = rep(paste0(all.start.year, "/",
                                           all.start.year+1),
                                    each = 100),
                       Day = rep(1:100, times = length(all.start.year)),
                       n = NA,
                       prop = NA)

k1 <- k2 <- 1
for (k1 in 1:length(all.start.year)){
  obs.day.1 <- obs.day[, k1]
  obs.n.1 <- obs.n[, k1]
  obs.day.1.uniq <- na.omit(unique(obs.day.1))
  for (k2 in 1:length(obs.day.1.uniq)){
    obs.n.df[obs.n.df$start.year == all.start.year[k1] &
               obs.n.df$Day == obs.day.1.uniq[k2], "n"] <- sum(obs.n[obs.day[,k1] == obs.day.1.uniq[k2], k1], na.rm = T)
    obs.n.df[obs.n.df$start.year == all.start.year[k1] &
               obs.n.df$Day == obs.day.1.uniq[k2], "prop"] <- sum(obs.prop[obs.day[,k1] == obs.day.1.uniq[k2], k1], na.rm = T)
  }
  
}

# This is for daily estimates
N.hats.day <- data.frame(Season = rep(paste0(all.start.year, 
                                             "/", all.start.year+1),
                                      
                                      each = nrow(jm.out$jm$mean$mean.N)), 
                         start.year = rep(all.start.year,
                                          each = nrow(jm.out$jm$mean$mean.N)),
                         Day = rep(1:nrow(jm.out$jm$mean$mean.N), 
                                   times = length(all.start.year)),
                         Mean = as.vector(jm.out$jm$mean$mean.N),
                         LCL = as.vector(jm.out$jm$q2.5$mean.N),
                         UCL = as.vector(jm.out$jm$q97.5$mean.N),
                         obs.n = obs.n.df$n,
                         prop = obs.n.df$prop,
                         Method = paste0("Eguchi M", ver)) %>%
  mutate(Nhat = obs.n/prop)

# Daily estimates plots
p.daily.Richards <- ggplot(N.hats.day %>% 
                             group_by(Season) %>%
                             arrange(by = Day)) + 
  geom_ribbon(aes(x = Day, ymin = LCL, ymax = UCL),
              fill = "blue", alpha = 0.5) +
  geom_path(aes(x = Day, y = Mean)) + 
  geom_point(aes(x = Day, y = Nhat), color = "darkgreen") +
  facet_wrap(~ Season)

save.fig.fcn(fig = p.daily.Richards, 
               file.name = "figures/Daily_Eguchi.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 10, width = 10, units = "in",
               bg = "white")

Reported.estimates <- read.csv(file = "Data/Nhats_2025.csv") %>%  
  transmute(Season = Season,
            Nhat = Nhat,
            LCL = LCL,
            UCL = UCL,
            Method = paste0(Method, "-Reported")) %>%
  right_join(all.years, by = "Season") %>%
  arrange(start.year) %>%
  relocate(Method, .after = start.year)
```

According to the bulk and tail ESS, and rank-normalized Rhat values, the MCMC for the best model converged well (Table x). Pareto-k statistics indicated that the model fit well to the data (Table x). Consequently, the posterior distributions of the parameters from the model `r ver` should be reliable. 

```{r Winbugs-results, echo=F,message=FALSE}
#WinBugs.run.date <- "2025-04-11"
WinBugs.out <- readRDS(file = paste0("RData/WinBUGS_2007to2025_v2_min", 
                                     min.dur,
                                     "_100000_",
                                     WinBUGS.Run.Date, ".rds"))

# Compute rank-normalized Rhat for WinBUGS output
BUGS.params <- "^lambda|^beta|^OBS.RF"
BUGS.samples <- WinBugs.out$BUGS.out$sims.array
BUGS.col.names <- grep(BUGS.params, dimnames(BUGS.samples)[[3]], 
                       value = T, perl = T)
subset.BUGS.samples <- BUGS.samples[,,BUGS.col.names]
subset.BUGS.mcmc.array <- as_draws_array(subset.BUGS.samples,
                                         .nchains = 5)

BUGS.Rhat <- apply(subset.BUGS.mcmc.array, MARGIN = 3, 
                   FUN = posterior::rhat)
BUGS.Rhat.big <- BUGS.Rhat[which(BUGS.Rhat > 1.01)]

BUGS.post <- posterior::as_draws(BUGS.samples)
summary.posterior.BUGS <- posterior::summarise_draws(BUGS.post)
summary.posterior.BUGS %>%
  select(variable, ess_bulk) %>%
  na.omit() %>%
  arrange(ess_bulk) -> ESS.bulk.BUGS

summary.posterior.BUGS %>%
  select(variable, ess_tail) %>%
  na.omit() %>%
  arrange(ess_tail) -> ESS.tail.BUGS

Corrected.Est <- WinBugs.out$BUGS.out$sims.list$Corrected.Est

# Create an observed counts dataframe
Bugs.obs.day <- WinBugs.out$BUGS.input$data$day
Bugs.obs.n <- WinBugs.out$BUGS.input$data$n[,1,]
Bugs.obs.n <- rbind(Bugs.obs.n, matrix(nrow = 2, ncol = ncol(Bugs.obs.n)))
Bugs.obs.prop <- WinBugs.out$BUGS.input$data$Watch.Length

Bugs.start.year <- WinBugs.out$BUGS.input$all.years - 1
Bugs.obs.n.df <- data.frame(start.year = rep(Bugs.start.year, 
                                             each = 90),
                            Season = rep(paste0(Bugs.start.year, "/",
                                                Bugs.start.year + 1),
                                         each = 90),
                            Day = rep(1:90, times = length(Bugs.start.year)),
                            n = NA,
                            Prop = NA)

k1 <- k2 <- 1
for (k1 in 1:length(Bugs.start.year)){
  obs.day.1 <- Bugs.obs.day[, k1]
  obs.n.1 <- Bugs.obs.n[, k1]
  obs.day.1.uniq <- na.omit(unique(obs.day.1)) 
  obs.day.1.uniq <- obs.day.1.uniq[obs.day.1.uniq > 1 & obs.day.1.uniq < 90]
  for (k2 in 1:length(obs.day.1.uniq)){
    Bugs.obs.n.df[Bugs.obs.n.df$start.year == Bugs.start.year[k1] &
                    Bugs.obs.n.df$Day == obs.day.1.uniq[k2], "n"] <- sum(Bugs.obs.n[Bugs.obs.day[,k1] == obs.day.1.uniq[k2], k1], 
                                                                         na.rm = T)
    Bugs.obs.n.df[Bugs.obs.n.df$start.year == Bugs.start.year[k1] &
                    Bugs.obs.n.df$Day == obs.day.1.uniq[k2], "Prop"] <- sum(Bugs.obs.prop[Bugs.obs.day[,k1] == obs.day.1.uniq[k2], k1], 
                                                                         na.rm = T)
  }
  
}

# We don't have raw data for 2006/2007 and 2007/2008 seasons
# Now we do. 2025-09-22
seasons <- c("2006/2007", "2007/2008",
             jm.out$jags.input$jags.input.new$seasons)

all.season <- paste0(all.start.year, "/", all.start.year+1)
Durban.abundance.df <- data.frame(Season = WinBugs.out$BUGS.input$seasons,
                                  Nhat = apply(Corrected.Est,
                                               FUN = mean,
                                               MARGIN = 2),
                                  # CV = apply(Corrected.Est,
                                  #            FUN = function(x) 100*sqrt(var(x))/mean(x),
                                  #            MARGIN = 2),
                                  # median = apply(Corrected.Est, 
                                  #                FUN = median, 
                                  #                MARGIN = 2),
                                  LCL = apply(Corrected.Est, 
                                              MARGIN = 2, 
                                              FUN = quantile,
                                              0.025),
                                  UCL = apply(Corrected.Est, 
                                              MARGIN = 2, 
                                              FUN = quantile,
                                              0.975)) %>%
  right_join(all.years, by = "Season") %>%
  arrange(start.year) %>%
  mutate(Method = "Durban")

# Create a dataframe for daily estimates:
daily.estim <- WinBugs.out$BUGS.out$sims.list$Daily.Est

# get stats:
mean.mat <- LCL.mat <- UCL.mat <- matrix(data = NA, 
                                         nrow = dim(daily.estim)[2], 
                                         ncol = dim(daily.estim)[3])

for (k1 in 1:dim(daily.estim)[2]){
  for (k2 in 1:dim(daily.estim)[3]){
    mean.mat[k1, k2] <- mean(daily.estim[,k1,k2])
    LCL.mat[k1, k2] <- quantile(daily.estim[,k1,k2], 0.025)
    UCL.mat[k1, k2] <- quantile(daily.estim[,k1,k2], 0.975)
  }
  
}

N.hats.day.Durban <- data.frame(Season = rep(WinBugs.out$BUGS.input$seasons,
                                             each = dim(daily.estim)[2]),
                                start.year = rep(WinBugs.out$BUGS.input$all.years - 1,
                                                 each = dim(daily.estim)[2]),
                                Day = rep(1:dim(daily.estim)[2], 
                                          length(WinBugs.out$BUGS.input$seasons)),
                                Mean = as.vector(mean.mat),
                                LCL = as.vector(LCL.mat),
                                UCL = as.vector(UCL.mat),
                                obs.n = Bugs.obs.n.df$n, 
                                prop = Bugs.obs.n.df$Prop,
                                Method = "Durban") %>%
  mutate(Nhat = obs.n/prop)
```

For the Durban et al's method, I found that the minimum bulk and tail ESS were too low (min(bulk ESS) = `r signif(min(ESS.bulk.BUGS$ess_bulk, digits = 2))`, min(tail ESS) = `r signif(min(ESS.tail.BUGS$ess_tail, digits = 2))`) to assume successful convergence of Markov chains. The rank-normalized Rhat statistics also indicated that convergence was not reached in `r length(BUGS.Rhat.big)` of `r dim(WinBUGS.out$BUGS.out$sims.array)[3]` monitored parameters (rank-normalized Rhat \> 1.01). Consequently, the inference from this model are questionable. 


```{r all-results, message=FALSE, echo=FALSE}

N.hats.day.all <- rbind(N.hats.day, N.hats.day.Durban) %>% 
  filter(start.year > 2005)

# Daily estimates plots
p.daily.Durban <- ggplot(N.hats.day.Durban %>% group_by(Season)) + 
  geom_ribbon(aes(x = Day, ymin = LCL, ymax = UCL),
              fill = "blue", alpha = 0.5) +
  geom_path(aes(x = Day, y = Mean)) + 
  geom_point(aes(x = Day, y = Nhat), color = "darkgreen") +
  facet_wrap(~ Season)

if (save.fig)
  save.fig.fcn(fig = p.daily.Durban, 
               file.name = "figures/Daily_Durban.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 5, width = 7, units = "in",
               bg = "white")

p.daily. <- ggplot(N.hats.day.all %>% group_by(Season)) + 
  geom_ribbon(aes(x = Day, ymin = LCL, ymax = UCL, fill = Method),
              alpha = 0.5) +
  geom_path(aes(x = Day, y = Mean, color = Method), size = 1) + 
  geom_point(aes(x = Day, y = Nhat, color = Method)) +
  facet_wrap(~ Season)

if (save.fig)
  save.fig.fcn(fig = p.daily., 
               file.name = "figures/Daily_Eguchi_Durban.png", 
               replace = T, 
               dpi = dpi.set, device = "png",
               height = 5, width = 7, units = "in",
               bg = "white")

# Include non-survey years - no estimates for 2007/2008 because I don't have
# raw data for that year. Only the WinBUGS inputs. 
Laake.abundance.new <- read.csv(file = "Data/all_estimates_Laake_2025_2025-09-22.csv") %>%
  mutate(LCL = CL.low,
         UCL = CL.high) %>%
  select(c(Season, Nhat, LCL, UCL)) %>%
  right_join(all.years, by = "Season") %>%
  arrange(start.year) %>%
  mutate(Method = "Laake")

#Laake.output <- read_rds(file = "RData/Laake_abundance_estimates_2024.rds")

# In reported estimates, there are two 2006/2007.
Reported.estimates %>%
  na.omit() %>%
  select(Season) %>% 
  unique() -> sampled.seasons 

# Reported estimates are not exactly identical to the reanalysis. For consistency,
# I use the reported estimates.  
# Laake.abundance.new %>%
#   rbind(Durban.abundance.df) %>%
Reported.estimates %>%
  mutate(Method1 = sub("-.*", "", Method)) -> tmp.1
tmp.1[grep("NA", tmp.1$Method1) & tmp.1$start.year < 2006, 
      "Method1"] <- "Laake"
tmp.1[grep("NA", tmp.1$Method1) & tmp.1$start.year > 2007, 
      "Method1"] <- "Durban"

tmp.1 %>%
  select(Season, Nhat, LCL, UCL, start.year, Method1) %>%
  rename(Method = Method1) %>% #-> tmp.2
  rbind(Laake.abundance.new %>% 
          filter(start.year > 2006)) %>%
  rbind(Nhat.) %>%
  mutate(Method2 = factor(Method, levels = c("Laake", 
                                             "Durban", 
                                             paste0("Eguchi M", ver)))) %>%
  select(Season, Nhat, LCL, UCL, start.year, Method2) %>%
  rename(Method = Method2) -> all.estimates

#  rbind(spline.Nhat) 
  #rbind(Reported.estimates %>% na.omit()) -> all.estimates

p.Nhats <- ggplot(all.estimates) +
  geom_point(aes(x = start.year, y = Nhat,
                 color = Method),
             alpha = 0.5) +
  geom_errorbar(aes(x = start.year, ymin = LCL, ymax = UCL,
                    color = Method)) +
  ylim(5000, 35000) +
  xlab("") + ylab("Abundance") +
  theme(legend.position = "top")

if (save.fig)
  save.fig.fcn(fig = p.Nhats,
               file.name = paste0("figures/Nhats_v", 
                                  ver, "_min", min.dur, 
                                  "_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)

Nhat. %>% 
  select(Season, start.year, Nhat, LCL, UCL) %>%
  rename(Nhat.Eguchi = Nhat,
         LCL.Eguchi = LCL,
         UCL.Eguchi = UCL) %>%
  cbind(Laake.abundance.new %>%
          select(Nhat, LCL, UCL) %>%
          rename(Nhat.Laake = Nhat,
                 LCL.Laake = LCL,
                 UCL.Laake = UCL)) %>%
  cbind(Durban.abundance.df %>%
          select(Nhat, LCL, UCL) %>%
          rename(Nhat.Durban = Nhat,
                 LCL.Durban = LCL,
                 UCL.Durban = UCL)) %>%
  mutate(d.Laake.Eguchi = Nhat.Laake - Nhat.Eguchi,
         d.Durban.Eguchi = Nhat.Durban - Nhat.Eguchi) -> Nhat.all.wide


# Compare how daily sums among years
# Check why I used watch.prop here. I have been using watch.length throughout the analysis
obsd.periods.primary <- jm.out$jags.input$jags.data$periods[,1]
watch.prop.primary <- jm.out$jags.input$jags.data$watch.prop[,1,]
obsd.effort.primary <- rbind(rep(0, times = dim(watch.prop.primary)[2]), 
                             watch.prop.primary, 
                             rep(0, times = dim(watch.prop.primary)[2]))

obsd.n.primary <- jm.out$jags.input$jags.data$n[,1,]
obsd.day.primary <- jm.out$jags.input$jags.data$day[,1,]
obsd.n.prop <- obsd.n.primary[,] * obsd.effort.primary

obsd.n.df <- data.frame(Season = rep(all.season, each = dim(obsd.n.prop)[1]),
                        obsd.n = as.vector(obsd.n.prop),
                        day = as.vector(obsd.day.primary),
                        effort = as.vector(obsd.effort.primary)) %>%
  na.omit()

# ggplot(obsd.n.df) +
#   geom_point(aes(x = day, y = obsd.n)) +
#   facet_wrap(~ Season)
# 
# ggplot(obsd.n.df) +
#   geom_point(aes(x = day, y = effort)) +
#   facet_wrap(~ Season)

#p.RF.Obs <- plot.trace.dens(jm.out$jm, "OBS.RF")

# if (save.fig){  
#   save.fig.fcn(fig = p.RF.Obs$p.trace, 
#                file.name = "figures/Observer_trace.png", 
#                replace = T, 
#                dpi = dpi.set, device = "png",
#                height = 10, width = 10, units = "in",
#                bg = "white")
#   
#   save.fig.fcn(fig = p.RF.Obs$p.dens, 
#                file.name = "figures/Observer_dens.png", 
#                replace = T, 
#                dpi = dpi.set, device = "png",
#                height = 10, width = 10, units = "in",
#                bg = "white")
#   
# }


# Daily estimates plots
# p.daily.Richards.1988 <- ggplot(N.hats.day.1988) + 
#   geom_ribbon(aes(x = Day, ymin = LCL, ymax = UCL),
#               fill = "blue", alpha = 0.5) +
#   geom_path(aes(x = Day, y = Mean)) + 
#   geom_point(aes(x = Day, y = Nhat), color = "darkgreen") 

```

For the Durban et al. and new approaches, medians of the posterior distributions were used as the point estimates, whereas for the Laake et al. approach, it was the maximum likelihood estimates. Estimated abundances from the new approach generally followed the same trend as the previous approaches, with a few notable differences (Figure `r officer::run_word_field("REF Figure-N-hats \\h")`). For the majority of estimates, the Laake et al. and the new approach provided comparable  estimates, and the 95% credible and confidence intervals overlapped significantly. The new and Laake et al. approaches produced lower estimates than the Durban et al. approach in all but one year among the 11 years when all three estimates were available.

```{r plot-N-hats, echo=FALSE, message=FALSE}

fig.Nhats.num <- run_autonum(seq_id = "fig",
                             pre_label = "Figure ",
                             bkm = "figure-N-hats")

knitr::include_graphics(paste0("figures/Nhats_v", 
                               ver, "_min", min.dur, "_", dpi.set, "dpi.png"))

```

:::{custom-style="Image Caption"}
`r fig.Nhats.num` Estimated annual abundance and their 95% CIs using Laake's method (blue, MLE), Durban's method (red, median), and the new approach (Eguchi M5, green, median). Laake's estimates prior to 2007 and Durban's estimates are found in Eguchi et al. (2025). Laake's estimates since the 2009/2010 season were computed using the scripts found in the *ERAnalysis* package."
:::

```{r gam-glm-fit, echo=F,message=FALSE}
all.estimates %>% na.omit() %>%
  mutate(time = start.year - min(start.year)) -> all.estimates.gam
  
all.estimates.gam %>%
  filter(Method == paste0("Eguchi M", ver)) %>% 
  filter(start.year > 2005) %>%
  mutate(year = time - min(time)) -> gam.estimates.2006.Eguchi


all.estimates.gam %>%
  filter(Method == paste0("Eguchi M", ver)) %>% 
  mutate(year = time - min(time)) -> gam.estimates.all.Eguchi

all.estimates.gam %>% 
  filter(Method == "Laake") %>% 
  filter(start.year > 2005) %>%
  mutate(year = time - min(time)) -> gam.estimates.2006.Laake

all.estimates.gam %>% 
  filter(Method == "Laake") %>% 
  mutate(year = time - min(time)) -> gam.estimates.all.Laake

all.estimates.gam %>%
  filter(Method == "Durban") %>% 
  filter(start.year > 2005) %>%
  mutate(year = time - min(time)) -> gam.estimates.2006.Durban

start.year.2006 <- min(gam.estimates.2006.Eguchi$start.year)

gam.2006.Eguchi <- mgcv::gam(Nhat ~ s(year), 
                        method = "REML", 
                        data = gam.estimates.2006.Eguchi )

gam.all.Eguchi <- mgcv::gam(Nhat ~ s(year), 
                        method = "REML", 
                        data = gam.estimates.all.Eguchi )

gam.2006.Laake <- mgcv::gam(Nhat ~ s(year), 
                       method = "REML",
                       data = gam.estimates.2006.Laake)

gam.all.Laake <- mgcv::gam(Nhat ~ s(year), 
                       method = "REML",
                       data = gam.estimates.all.Laake)

gam.2006.Durban <- mgcv::gam(Nhat ~ s(year), 
                        method = "REML",
                        data = gam.estimates.2006.Durban)

pred.data.2006 <- data.frame(year = seq(min(gam.estimates.2006.Eguchi$year),
                                        max(gam.estimates.2006.Eguchi$year),
                                        length.out = 200))

pred.data.all <- data.frame(year = seq(min(gam.estimates.all.Eguchi$year), 
                                        max(gam.estimates.all.Eguchi$year),
                                       length.out = 1000))

gam.preds.2006.Eguchi <-predict(gam.2006.Eguchi, 
                                newdata = pred.data.2006, 
                                se.fit = TRUE)
gam.preds.all.Eguchi <-predict(gam.all.Eguchi, 
                               newdata = pred.data.all, 
                               se.fit = TRUE)

gam.preds.2006.Laake <-predict(gam.2006.Laake, 
                               newdata = pred.data.2006, 
                               se.fit = TRUE)
gam.preds.all.Laake <-predict(gam.all.Laake, 
                              newdata = pred.data.all, 
                              se.fit = TRUE)

gam.preds.2006.Durban <-predict(gam.2006.Durban, 
                                newdata = pred.data.2006, 
                                se.fit = TRUE)

# Combine predictions with the x values and calculate the confidence interval
pred.data.2006 %>%
  mutate(fit = gam.preds.2006.Eguchi$fit,
         SE = gam.preds.2006.Eguchi$se.fit,
         upper_CI = gam.preds.2006.Eguchi$fit + (1.96 * SE),
         lower_CI = gam.preds.2006.Eguchi$fit - (1.96 * SE),
         Method = paste0("Eguchi M", ver)) -> pred.2006.Eguchi

pred.data.2006 %>%
  mutate(fit = gam.preds.2006.Laake$fit,
         SE = gam.preds.2006.Laake$se.fit,
         upper_CI = gam.preds.2006.Laake$fit + (1.96 * SE),
         lower_CI = gam.preds.2006.Laake$fit - (1.96 * SE),
         Method = "Laake") -> pred.2006.Laake

pred.data.2006 %>%
  mutate(fit = gam.preds.2006.Durban$fit,
         SE = gam.preds.2006.Durban$se.fit,
         upper_CI = gam.preds.2006.Durban$fit + (1.96 * SE),
         lower_CI = gam.preds.2006.Durban$fit - (1.96 * SE),
         Method = "Durban") -> pred.2006.Durban

pred.2006 <- rbind(pred.2006.Eguchi, pred.2006.Durban, pred.2006.Laake) %>%
  rename(Method1 = Method) %>%
  mutate(Method = factor(Method1, 
                         levels = c("Laake", 
                                    "Durban",
                                    paste0("Eguchi M", ver))))
  #mutate(method.f = as.factor(method))

pred.data.all %>%
  mutate(fit = gam.preds.all.Eguchi$fit,
         SE = gam.preds.all.Eguchi$se.fit,
         upper_CI = gam.preds.all.Eguchi$fit + (1.96 * SE),
         lower_CI = gam.preds.all.Eguchi$fit - (1.96 * SE),
         Method = paste0("Eguchi M", ver)) -> pred.all.Eguchi

pred.data.all %>%
  mutate(fit = gam.preds.all.Laake$fit,
         SE = gam.preds.all.Laake$se.fit,
         upper_CI = gam.preds.all.Laake$fit + (1.96 * SE),
         lower_CI = gam.preds.all.Laake$fit - (1.96 * SE),
         Method = "Laake") -> pred.all.Laake

pred.all <- rbind(pred.all.Eguchi, pred.all.Laake) %>%
  rename(Method1 = Method) %>%
  mutate(Method = factor(Method1, 
                         levels = c("Laake", paste0("Eguchi M", ver))))


p.gam.2006.fit <- ggplot(data = pred.2006) +
  # Add the confidence interval ribbon using the new prediction data
  geom_ribbon(aes(x = year, 
                  ymin = lower_CI,
                  ymax = upper_CI, 
                  fill = Method), 
              alpha = 0.3) +
  
  # Add the fitted line using the new prediction data
  geom_line(aes(x = year, y = fit, color = Method), 
            size = 1) +
  scale_x_continuous(labels = function(breaks){ paste0(breaks + start.year.2006)}) +
  xlab("") + ylab("Abundance") +
  theme(legend.position = "top")
  
p.gam.all.fit <- ggplot(data = pred.all) +
  # Add the confidence interval ribbon using the new prediction data
  geom_ribbon(aes(x = year, ymin = lower_CI, ymax = upper_CI, fill = Method), 
              alpha = 0.3) +
  
  # Add the fitted line using the new prediction data
  geom_line(aes(x = year, y = fit, color = Method), 
            size = 1) +
  scale_x_continuous(labels = function(breaks){ paste0(breaks + 1967)}) +
  xlab("") + ylab("Abundance") +
  theme(legend.position = "top")

  # geom_point(data = all.estimates.gam %>%
  #              filter(start.year > 2005),
  #            aes(x = time, y = Nhat, color = Method)) +  # Plot the raw data points from the original dataframe
  

if (save.fig)
  save.fig.fcn(fig = p.gam.2006.fit,
               file.name = paste0("figures/gam_fit_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)

lm.Eguchi <- lm(log(Nhat) ~ time, 
                data = all.estimates.gam %>% 
                  filter(start.year > 2013, 
                         Method == paste0("Eguchi M", ver)))

lm.Laake <- lm(log(Nhat) ~ time, 
               data = all.estimates.gam %>% 
                 filter(start.year > 2013, 
                        Method == "Laake"))

lm.Durban <- lm(log(Nhat) ~ time, 
                data = all.estimates.gam %>%
                  filter(start.year > 2013, 
                         Method == "Durban"))

```

GAMs fitted to the estimated abundance between the 2006/2007 and 2024/2025 seasons indicated that all three approaches provided comparable results regarding the general trend, with a main difference at the beginning of the timeseries (Figure `r officer::run_word_field("REF Figure-gam-fits \\h")`). For the 2006/2007 season, the estimated abundance with the new approach was notably lower than that from the other two methods (Figure `r officer::run_word_field("REF Figure-N-hats \\h")`). Even with this difference, however, estimated intercepts (the average of abundance estimates) between the new (`r format(coef(gam.2006.Eguchi)["(Intercept)"], scientific = F)` $\pm$ `r signif(summary.gam(gam.2006.Eguchi)$se["(Intercept)"], 4)` (SE)) and the Laake et al. approach (`r format(coef(gam.2006.Laake)["(Intercept)"], scientific = F)` $\pm$ `r signif(summary.gam(gam.2006.Laake)$se["(Intercept)"], 4)` (SE)) were not significantly different, whereas the Durban approach was significantly greater (`r format(coef(gam.2006.Durban)["(Intercept)"], scientific = F)` $\pm$ `r signif(summary.gam(gam.2006.Durban)$se["(Intercept)"], 4)` (SE)) than the other two approaches. Over the entire timeseries from the 1967/1968 season, the estimated intercepts were similar between the two approaches (Laake: `r format(coef(gam.all.Laake)["(Intercept)"], scientific = F)` $\pm$ `r signif(summary.gam(gam.all.Laake)$se["(Intercept)"], 4)` (SE), Eguchi: `r format(coef(gam.all.Eguchi)["(Intercept)"], scientific = F)` $\pm$ `r signif(summary.gam(gam.all.Eguchi)$se["(Intercept)"], 4)` (SE))

For general linear models ($log(\hat{N}) = time + \epsilon$) fitted to the point estimates from the 2014/2015 season to the 2024/2025 season revealed a statistically indistinguishable annual rate of decline among the three approaches (Laake: `r signif(lm.Laake$coefficients[2],1)` $\pm$ `r signif(coef(summary(lm.Laake))["time", "Std. Error"], 2)` (SE). Durban: `r signif(lm.Durban$coefficients[2],1)` $\pm$ `r signif(coef(summary(lm.Durban))["time", "Std. Error"], 2)` (SE), Eguchi: `r signif(lm.Eguchi$coefficients[2],1)` $\pm$ `r signif(coef(summary(lm.Eguchi))["time", "Std. Error"], 2)` (SE)).

```{r plot-gam-fits, echo=FALSE, message=FALSE}

fig.GAM.num <- run_autonum(seq_id = "fig", 
                           pre_label = "Figure ", 
                           bkm = "Figure-gam-fits")

knitr::include_graphics(paste0("figures/gam_fit_", dpi.set, "dpi.png"))
```

::: {custom-style="Image Caption"} 
`r fig.GAM.num` Fitted generalized additive models (Gaussian family and the identity link function) to estimated abundance and their approximate 95% CIs."
:::

In this new approach, I did not have the mean detection probability as in the Durban's approach. Instead, the detection probability was estimated as a function of the fixed observer effect, Beaufort sea state, and visibility condition. As expected, the fixed effect of the visibility code on observation probability was negative (Figure `r officer::run_word_field("REF Figure-Fixed-posteriors \\h")`), indicating that less favorable conditions resulted in lower detection probabilities. Perplexingly, the fixed effect of the Beaufort sea state was positive (Figure `r officer::run_word_field("REF Figure-Fixed-posteriors \\h")`), suggesting that higher Beaufort sea states corresponded with greater detection probabilities. One possible explanation is that observers may have detected more singletons at low sea states, whereas only large groups were sighted at high sea states. The observer fixed effects indicated significant differences among individuals, consistent with the findings of Durban et al. (Appendix Figure `r officer::run_word_field("REF Figure-observers \\h")`).

```{r posterior-fixed-effects-plots, echo=FALSE, message=FALSE}

posteriors.Fixed <- plot.trace.dens(jm.out$jm, 
                                    var.name = ".Fixed")

posteriors.Fixed$df %>%
  mutate(par.name.new = ifelse(par.name == "VS.Fixed", "V", "B")) %>%
  mutate(par.name.new.f = factor(par.name.new, 
                                 levels = c("V", "B"))) -> VS.BF.df

custom_labels <- c(`V` = "beta[V]",
                   `B` = "beta[B]")

p.posteriors.VS.BF <- ggplot(VS.BF.df) +
  geom_density(aes(x = sample)) +
  facet_wrap(~ par.name.new.f, scales = "free_x", 
             labeller = as_labeller(custom_labels, default = label_parsed)) +
  xlab("") + ylab("Density")

# p.posteriors.Fixed <- ggplot(posteriors.Fixed$df) +
#   geom_density(aes(x = sample)) +
#   facet_wrap(~ par.name.ordered, scales = "free_x") +
#   xlab("") + ylab("Density")

if (save.fig)
  save.fig.fcn(fig = p.posteriors.VS.BF,
               file.name = paste0("figures/posteriors_VS_BF_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)

# Look into why BF.Fixed is negative.
n.mat <- jm.out$jags.input$jags.data$n[,1,]
bf.mat <- rbind(matrix(data = NA, nrow = 1, ncol = length(all.start.year)),
                       jm.out$jags.input$jags.data$bf[,1,],
                matrix(data = NA, nrow = 1, ncol = length(all.start.year)))
                
day.mat <- jm.out$jags.input$jags.data$day[,1,]

n.and.bf <- data.frame(n = n.mat[day.mat > 1 & day.mat < 100],
                       bf = bf.mat[day.mat > 1 & day.mat < 100])

```


```{r plot-Fixed-posteriors, echo=FALSE, message=FALSE}

fig.fixed.post.num <- run_autonum(seq_id = "fig", 
                                  pre_label = "Figure ", 
                                  bkm = "Figure-Fixed-posteriors")

knitr::include_graphics(paste0("figures/posteriors_VS_BF_", dpi.set, "dpi.png"))

```

:::{custom-style="Image Caption"} 
`r fig.fixed.post.num` Posterior distributions of the fixed-effects coefficients for the visibility code ($\beta_{V}$) and Beaufort sea state ($\beta_{B}$).
:::

Posterior distributions of the Richards equation parameters provided valuable insights into how annual gray whale migration varied among years. The $P$ parameters, which define the timing of the migration peak, shifted over the years (Figure `r officer::run_word_field("REF Figure-P-posteriors \\h")`), a phenomenon reported in previous studies [@rugh_timing_2001], Eguchi et al. 2023, 2024, 2025), although the significance and causes for the changes are unknown. The $S_1$ parameter, which define the increasing part of the migration curve, also varied annually, indicating that the way the number of migrating whales increased over time fluctuated from year to year (Appendix Figure `r officer::run_word_field("REF Figure-S1-posteriors \\h")`. On the other hand, the best model included only one common $S_2$ parameter among all seasons, indicating the decline of the number of migrating whales was similar among years `r officer::run_word_field("REF Figure-S2-posteriors \\h")`), or insufficient counts were available towards the latter half of migration.

```{r posterior-Richards-plots, echo=FALSE, message=FALSE}

posteriors.P <- plot.trace.dens(jm.out$jm, 
                                var.name = "^P\\[")

posteriors.P$df %>%
  mutate(start.year = all.start.year[numeric.index],
         start.year.f = as.factor(start.year)) %>% 
  rename(P = sample)-> P.df

p.posteriors.P <- ggplot(P.df, aes(x = P, y = start.year.f)) +
  geom_density_ridges() +
  ylab("Start year")

if (save.fig)
  save.fig.fcn(fig = p.posteriors.P,
               file.name = paste0("figures/posterior_P_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)

# Need to include \\[ to remove S1.alpha and S1.beta
posteriors.S1 <- plot.trace.dens(jm.out$jm,
                                 var.name = "S1\\[")

posteriors.S1$df %>%
  mutate(start.year = all.start.year[numeric.index],
         start.year.f = as.factor(start.year)) %>% 
  rename(S1 = sample)-> S1.df

p.posteriors.S1 <- ggplot(S1.df, aes(x = S1, y = start.year.f)) +
  geom_density_ridges() +
  xlim(c(4, 25)) +
  ylab("Start year")

if (save.fig)
  save.fig.fcn(fig = p.posteriors.S1,
               file.name = paste0("figures/posteriors_S1_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)
 
p.posterior.S2 <- mcmc_dens(jm.out$jm$samples, "S2")

#mcmc_trace(jm.out$jm$samples, "S2")

# posteriors.S2 <- plot.trace.dens(jm.out$jm,
#                                  var.name = "^S2\\b(?!\\.\\w*)")
# 
# posteriors.S2$df %>%
#   mutate(start.year = all.start.year[numeric.index],
#          start.year.f = as.factor(start.year)) %>% 
#   rename(S2 = sample)-> S2.df
# 
# p.posteriors.S2 <- ggplot(S2.df, aes(x = S2, y = start.year.f)) +
#   geom_density_ridges() +
#   ylab("")

if (save.fig)
  save.fig.fcn(fig = p.posterior.S2,
               file.name = paste0("figures/posteriors_S2_", dpi.set, "dpi.png"),
               replace = T,
               height = 5, width = 7,
               dpi = dpi.set)


```

```{r plot-P-posteriors, echo=FALSE, message=FALSE} 

fig.P.num <- run_autonum(seq_id = "fig", 
                         pre_label = "Figure ", 
                         bkm = "Figure-P-posteriors")

knitr::include_graphics(paste0("figures/posterior_P_", dpi.set, "dpi.png"))


```

:::{custom-style="Image Caption"} 
`r fig.P.num` Posterior distributions of year-specific P parameters.
:::

<!-- The daily estimated mean between Durban et al's and the new approaches was similar qualitatively (Figure @ref(fig:Figure-daily-N-hats)), indicating the two approaches were similar. Credible intervals were generally narrower for the new approach than Durban et al.'s. -->

<!-- ```{r Figure-daily-N-hats, echo=FALSE, message=FALSE, fig.cap = "Estimated mean daily abundance and their 95% CIs using Durban et.al's (red) and the new approach (green). The solid lines indicate the mean. Circles indicate the number of observed whales divided by the proprotion of day observed. For the 2006/2007 season, the new approach was applied to the data from the *ERAnalysis* package, whereas WinBUGS was ran on the input structure in Durban's analysis with 90 minutes as the minimum shift duration. Consequently, the observed counts are different between the two approaches. For the 2007/2008 season, no raw data were available. Therefore, the new approach was not applied to the data. "} -->

<!-- knitr::include_graphics("figures/Daily_Eguchi_Durban.png") -->

<!-- ``` -->

## Discussion {.unnumbered}

The new modeling approach yielded results comparable to those obtained by the methods of Laake et al. and Durban et al. Although point estimates were different in some years, the confidence and credible intervals overlapped in most of the time series, and all three methods captured similar long-term trends and the recent rate of decline.

The new approach offers several advantages over the method of Durban et al. First, it eliminates the use of the “cut” function, which has been identified as statistically inappropriate (Plummer 2015). Second, the model is coded in JAGS, a more contemporary and widely used Bayesian modeling language. The new model in JAGS runs significantly faster than Durban et al.'s model in WinBUGS. The analysis of the entire 32-year dataset took approximately `r signif(jm.out$Run_Time, 2)` hrs per model with the new approach using longer chains (`r jm.out$MCMC.params$n.samples` samples), compared to `r signif(WinBugs.out$Run_Time, 3)` hours for an 11-year analysis using WinBUGS (100000 samples). Third, the new framework allows for formal model selection and goodness-of-fit assessment using modern criteria, such as LOOIC and the Pareto-k statistic, which were not part of the original Durban et al. analysis. Fourth, the metrics that measure convergence of MCMC indicated that the Durban et al's approach did not converge well, whereas the new method did.  

No data point was flagged as lack-of-fit using the new method based on Pareto-k statistics, indicating that all data points fit well to the model. Thus the model, i.e., combining the Richards equation and the hierarchical state-space modeling framework, was appropriate for analyzing the entire dataset. With decreasing funding and logistical challenges, having two simultaneous independent observers has become difficult, if not impossible. Consequently, the modeling framework has to adapt to this reality. The proposed new approach achieved the goal of estimating abundance precisely from one observer. However, it is advisable to have two observers in some years in order to increase the precision.

The new approach is also adaptable and can incorporate additional information as it becomes available. For example, data from uncrewed aircraft systems (UAS) are being used to estimate precision and bias in group size estimates made by visual observers. If correction functions can be derived from these UAS data, conditional on observed group sizes, they could be used to adjust the group size of each sighting so that the total number of sightings per watch period can be adjusted accordingly. Finally, this abundance estimation model could be incorporated into a more comprehensive population model, such as the one developed by [@stewart_boom-bust_2023].

In summary, the new approach provides a simpler, faster, and more statistically robust algorithm for estimating the abundance of gray whales from the long-term visual surveys conducted at Granite Canyon, CA, while producing results that are consistent with those of previous methods. I think the new method should be used for future abundance estimation of the Eastern Pacific gray whales. 

## Appendix {.unnumbered}

### Mathematical description of Durban's approach {.unnumbered}

This section was extracted from Durban et al. (2015), almost verbatim. I edited some places to make explanations clearer and changed some symbols to make them consistent between theirs and the new approach.

The total counts of whales ($n_{i,d(j),t}$) during the watch period $j$ of the $d$th day in the year $t$ at the watch station $i$ was modeled as a binomial random deviate (in the paper, $j$ was not specified as the $j$th watch period during the $d$th day of the season):

$$ n_{i,d(j),t} \sim BIN(N_{d(j),t}, p_{i,d(j),t}). $$

The binomial $N_{d(j),t}$ parameter is the unknown total number of gray whales passing through the study area during the watch period $j$ in the $d$th day in the year $t$.

The detection probability was modeled as a function of visibility (V), Beaufort scale (B), and observers (O).

$$ logit(p_{i,d(j),t}) = logit(p_0) + I_{V} \beta_{V} V_{d(j),t} + I_{B} \beta_{B} B_{d(j),t}+ I_{O} \mathbf{\beta}^{O_{i,d(j),t} = o}_{O} $$

where the intercept $p_0$ was the base detection probability in the absence of covariate effects, assigned a Uniform(0,1) prior distribution, and $logit(p_0) = ln(p_0/(1–p_0))$. For each fixed effect, $\beta_{B}$ and $\beta_{V}$, a Normal prior distribution with mean zero and standard deviation of 10 was used. The random effect for each observer was drawn from a Normal distribution with mean zero and standard deviation $\sigma_{O} \sim Uniform(0,10)$. Each binary indicator variable, $I$, was assigned a Bernoulli(0.5) distribution to specify equal probability of inclusion (1) or exclusion (0) of the effect in the model (Durban et al. 2015).

A Poisson distribution ($N_{d(j),t} \sim Poisson(\lambda_{d(j),t})$) was used as a hierarchical prior for the distribution of abundances, and specified a model for the Poisson mean ($\lambda$) in terms of the number of whales passing each day ($d$), with an offset for the effort duration of each watch period, $E_{d(j),t}$ in decimal days (Laake et al., 2012):

$$ log(\lambda_{d(j),t}) = log(E_{d(j),t}) + model_{d,t}$$

$$ model_{d,t} = z_{d,t} f_{d,t} + (1 – z_{d,t}) g_{d,t} $$

where $z_{d,t}$ is an indicator function (see below). $f_{d,t}$ and $g_{d,t}$ are also explained below.

Days were specified as d = 0 to 90, where days were counted from 12:00 a.m. on 1 December, and we added an abundance of 0 whales passing for day 0 and 90 to anchor the fitted model (Buckland et al. 1993).

Estimates were derived from either of two competing models ('Common' ($f$) and 'Specific' ($g$), e.g., [@li_baystdetect_2012]), describing changes in abundance across each annual migration. The model contributing each daily estimate was indicated using stochastic binary indicator variables $z_{d,t}$, each assigned a Bernoulli(0.5) prior distribution. As such, the posterior distribution of each $z_{d,t}$ indicated the probability of a daily estimate conforming to the common trend, allowing flexibility for departures from this trend that may only exist on certain days in certain years to be identified and modeled (rather than assuming all counts from an entire year conform to or depart from a common trend). The total number of whales passing at the survey location during each migration was then estimated by summing the expected value from the model averaged number of whales passing each day ($d$) from 0 to 90 (Laake et al., 2012).

These estimates were then rescaled to account for the differential passage rate at night [@perryman_diel_1999], based on the nine hour day multiplicative correction factor [@rugh_estimates_2005]. Specifically, we applied a constant night time correction factor that was assumed to be a Normally distributed fixed effect with mean of 1.0875 and standard deviation of 0.037 [@perryman_diel_1999].

For the 'Common model' ($f_{d,t}$), we assumed a typical trend in abundance throughout each annual migration (e.g. Buckland et al., 1993), with abundance changes assumed Normally distributed around a migration mid-point. A Normal distribution was specified as a quadratic function of days, on the log scale:

$$f_{d,t} = a_t + b_t *d_t + c_t * d^2_t$$

where the mid-point of the migration curve for each year $t$ was derived by $–b_t/2a_t$. This assumed common migration curve allowed information to be 'borrowed' across years when needed, specifying association across years to strengthen inference about migration curves in years with relatively sparse counts. Each parameter was specified to be drawn from a hierarchical Normal distribution.

$$ a_t \sim Normal(\mu_a, \sigma_a)$$

$$ b_t \sim Normal(\mu_b, \sigma_b)$$

$$ c_t \sim Normal(\mu_c, \sigma_c)$$

and $\mu \sim Normal(0, 10)$ and $\sigma \sim Uniform(0, 10)$, for $a$, $b$, and $c$.

This hierarchical and random effects approach allowed the timing, level and extent of the Normal migration curve to vary annually around the general pattern, if supported by the data.

To acknowledge and incorporate deviations from the common Normal model, the selection of an alternative 'specific' migration model was allowed ($g_{d,t}$). The 'specific' model was a semi-parametric model that estimated the time trends independently for each year without making any prior assumptions about its form (e.g., Laake et al., 2012). In this model, the shape of the relationship of true abundance across days was determined by the data via penalized splines [@ruppert_selecting_2002].

A linear (on the log scale) penalized spline was used to describe this relationship [@crainiceanu_bayesian_2005]:

$$ g_{d,t} = S_{0,t} + S_{1,t} * d_t + \sum_{k=1}^{m} \lambda_{k,t} (d_t – \kappa_{k,t}) $$ Where $S_{0,t}, S_{1,t}$ and $\lambda_{1,t}, \dots, \lambda_{m,t}$ were regression coefficients to be estimated separately for each year and $\kappa_{1,t} < \kappa_{2,t} < \dots < \kappa_{m,t}$ were fixed knots. To ensure the desired flexibility, we used m = 15 knots, which is a relatively large number. To avoid overfitting, the $\lambda$'s were penalized by assuming that they were Normally distributed random variables with mean 0 and standard deviation $\sim Uniform(0,10)$. The parameters $S_{0,t}, S_{1,t}$ were modeled as fixed effects with Normal(0, 10) prior distributions.

### Characteristics of the Richards equation {.unnumbered}

#### Effects of $S_1$ {.unnumbered}

The parameter $S_1$ defines how the curve decreases from its peak. The rate of decline slows down as $S_1$ becomes greater (Figure A`r officer::run_word_field("REF Figure-S1 \\h")`). Because we make an assumption that there are no whales migrating after day 100 (or any other assumed "end" day), the lower bound of $S_1$ can be restricted to a certain value, (e.g., $0 < S_1 < 5$).

```{r S1, echo=FALSE, message=FALSE}
S1 <- c(10, 5, 2.5, 1.2, 0.6, 0.3)
S2 <- 1.5
K <- 1
P <- 40
max.N <- 800

true.mean.N <- matrix(data = 0, nrow = 90, ncol = length(S1))

for (c in 1:length(S1)){
  for (d in 1:90){
    true.mean.N[d, c] <- floor(Richards_fcn(d = d, 
                                            S1 = -S1[c], 
                                            S2 = S2,
                                            K = K, 
                                            P = P, 
                                            min = 0, max = max.N)  )
    

  }
  
}

data.df <- data.frame(Day = rep(1:90, times = length(S1)),
                      mean.N = as.vector(true.mean.N),
               
                      S1 = rep(S1, each = 90))

p.S1 <- ggplot(data = data.df) +
  geom_path(aes(x = Day, y = mean.N), color = "red") +
  facet_wrap(~ S1) +
  ylab("")

save.fig.fcn(p.S1, 
             file.name = paste0("figures/S1_", 
                                dpi.set, "dpi.png"), 
             height = 5, width = 7,replace = T, dpi = dpi.set)

```

```{r plot-S1, echo=FALSE, message=FALSE}
# fig.cap=paste0("Effects of $S_1$ on the shape of the Richards equation. In this example, $S_2$ =", S2, ", K = ", K, ", P = ", P, ", $N_{max}$ = ", max.N, ".")
# 
fig.S1.num <- run_autonum(seq_id = "fig", 
                          pre_label = "Figure A", 
                          bkm = "Figure-S1",
                          start_at = 1)

knitr::include_graphics(paste0("figures/S1_", dpi.set, "dpi.png"))

```

:::{custom-style="Image Caption"}
`r fig.S1.num` Effects of $S_1$ on the shape of the Richards equation. In this example, $S_2$ = `r S2`, K = `r K`, P = `r P`, $N_{max}$ = `r max.N`.
:::

#### Effects of $S_2$ {.unnumbered}

The parameter $S_20$ defines how the curve increases to its peak. The rate of increase slows down as $S_2$ becomes larger (Figure A`r officer::run_word_field("REF Figure-S2 \\h")`). Because we make an assumption that there are no whales migrating before day 1, the upper bound of $S_2$ can be restricted to a certain value (e.g., $0 < S_2 < 5$).

```{r S2, echo=FALSE, message=FALSE}
S1 <- 1.5
S2 <- c(0.3, 0.6, 1.2, 2.5, 5, 10)
K <- 1
P <- 40
max.N <- 800

true.mean.N <- matrix(data = 0, nrow = 90, ncol = length(S2))

for (c in 1:length(S2)){
  for (d in 1:90){
    true.mean.N[d, c] <- floor(Richards_fcn(d = d, 
                                            S1 = -S1, 
                                            S2 = S2[c],
                                            K = K, 
                                            P = P, 
                                            min = 0, max = max.N)  )
    
  }
  
}

data.df <- data.frame(Day = rep(1:90, times = length(S2)),
                      mean.N = as.vector(true.mean.N),

                      S2 = rep(S2, each = 90))

p.S2 <- ggplot(data = data.df) +
  geom_path(aes(x = Day, y = mean.N), color = "red") +
  facet_wrap(~ S2) +
  ylab("")

save.fig.fcn(p.S2, 
             file.name = paste0("figures/S2_", dpi.set, "dpi.png"), 
             height = 5, width = 7,replace = T, dpi = dpi.set)


```

```{r plot-S2, echo=FALSE, message=FALSE}

fig.S2.num <- run_autonum(seq_id = "fig", 
                          pre_label = "Figure A", 
                          bkm = "Figure-S2")

knitr::include_graphics(paste0("figures/S2_", dpi.set, "dpi.png"))

```

:::{custom-style="Image Caption"}
`r fig.S2.num` Effects of $S_2$ on the shape of the Richards equation. In this example, $S_1$ = `r S1`, K = `r K`, P = `r P`, $N_{max}$ = `r max.N`.
:::

#### Effects of K {.unnumbered}

The parameter $K > 0$ defines the flatness of the curve at its peak. Greater $K$ values correspond to flatter peaks (Figure A`r officer::run_word_field("REF Figure-K \\h")`). Similarly to $S_1$ and $S_2$, the upper bound of $K$ may be defined based on the assumption that the numbers of migrating gray whales are zero at day 1 and 100 (e.g., $0 < K < max(K)$). The upper bound ($max(K)$) may change when other parameters change, such as $S_1$ and $S_2$.

```{r K, echo=FALSE, message=FALSE}
S1 <- 2.5
S2 <- 2.5
K <- c(0.01, 0.1, 1, 2, 4, 8)
P <- 40
max.N <- 800

true.mean.N <- matrix(data = 0, nrow = 90, 
                           ncol = length(K))

for (c in 1:length(K)){
  for (d in 1:90){
    true.mean.N[d, c] <- floor(Richards_fcn(d = d, 
                                            S1 = -S1, 
                                            S2 = S2,
                                            K = K[c], 
                                            P = P, 
                                            min = 0, max = max.N)  )
    
  }
  
}

data.df <- data.frame(Day = rep(1:90, times = length(K)),
                      mean.N = as.vector(true.mean.N),
                      K = rep(K, each = 90))

p.K <- ggplot(data = data.df) +
  geom_path(aes(x = Day, y = mean.N), color = "red") +
  facet_wrap(~ K) +
  ylab("")

save.fig.fcn(p.K, 
             file.name = paste0("figures/K_", dpi.set, "dpi.png"), 
             height = 5, width = 7,replace = T, dpi = dpi.set)



```

```{r plot-K, echo=FALSE, message=FALSE}

fig.K.num <- run_autonum(seq_id = "fig", 
                                    pre_label = "Figure A",
                                    bkm = "Figure-K")

knitr::include_graphics(paste0("figures/K_", dpi.set, "dpi.png"))

```

:::{custom-style="Image Caption"}
`r fig.K.num` Effects of $K$ on the shape of the Richards equation. In this example, $S_1$ = `r S1`, $S_2$ = `r S2`, P = `r P`, $N_{max}$ = `r max.N`.
:::

#### Effects of P {.unnumbered}

The parameter $P$ defines the location of its peak (Figure A`r officer::run_word_field("REF Figure-P \\h")`).

```{r P, echo=FALSE, message=FALSE}
S1 <- 2.5
S2 <- 2.5
K <- 1.5
P <- c(20, 40, 60, 80)
max.N <- 800

true.mean.N <- matrix(data = 0, nrow = 90, 
                           ncol = length(P))

for (c in 1:length(P)){
  for (d in 1:90){
    true.mean.N[d, c] <- floor(Richards_fcn(d = d, 
                                            S1 = -S1, 
                                            S2 = S2,
                                            K = K, 
                                            P = P[c], 
                                            min = 0, max = max.N)  )
    
  }
  
}

data.df <- data.frame(Day = rep(1:90, times = length(P)),
                      mean.N = as.vector(true.mean.N),
                      P = rep(P, each = 90))

p.P <- ggplot(data = data.df) +
  geom_path(aes(x = Day, y = mean.N), color = "red") +
  facet_wrap(~ P) +
  ylab("")

save.fig.fcn(p.P, 
             file.name = paste0("figures/P_", 
                                dpi.set, "dpi.png"), 
             replace = T, 
             height = 5, width = 7,dpi = dpi.set)


```

```{r plot-P, echo=FALSE, message=FALSE}

fig.P.num <- run_autonum(seq_id = "fig", 
                                    pre_label = "Figure A",
                                    bkm = "Figure-P")

knitr::include_graphics(paste0("figures/P_", dpi.set, "dpi.png"))

```

:::{custom-style="Image Cpation"}
`r fig.P.num` Effects of $P$ on the shape of the Richards equation. In this example, $S_1$ = `r S1`, $S_2$ = `r S2`, K = `r K`, $N_{max}$ = `r max.N`.
:::

### Hyperparameter definitions {.unnumbered}
```{r table-hyperparmaters-setup, echo=FALSE, message=FALSE}
tab.hyp.num <- run_autonum(seq_id = "tabA", 
                           pre_label = "", 
                           bkm = "hyperparameter-table",
                           start_at = 1)
```

:::{custom-style="Image Cpation"}
Table A`r tab.hyp.num` Definitions of hyperdistributions and their parameters.
:::
```{r table-hyperparameters, echo=FALSE, message=FALSE}

ft.hyper.params %>%
  set_table_properties(layout = "autofit") %>%
  #autofit() %>%
  paginate(init = TRUE, hdr_ftr = TRUE) 

```


### Trace plots of parameters with high rank-standardized `r R_hat` values {.unnumbered}

```{r plot-trace-1, echo=FALSE, message=FALSE}

fig.trace.1.num <- run_autonum(seq_id = "fig", 
                               pre_label = "Figure A",
                               bkm = "Figure-trace-1")

knitr::include_graphics(paste0("figures/M5_trace_1.png"))


```

:::{custom-style="Image Caption"}
`r fig.trace.1.num` Trace plots of parameters with high `r R_hat` values.
:::

```{r plot-trace-2, echo=FALSE}
fig.trace.2.num <- run_autonum(seq_id = "fig", 
                               pre_label = "Figure A",
                               bkm = "Figure-trace-2")

knitr::include_graphics(paste0("figures/M5_trace_2.png"))

```

:::{custom-style="Image Caption"}
`r fig.trace.2.num` Trace plots of parameters with high `r R_hat` values.
:::

```{r plot-trace-3, echo=FALSE, message=FALSE}
fig.trace.3.num <- run_autonum(seq_id = "fig", 
                               pre_label = "Figure A",
                               bkm = "Figure-trace-3")

knitr::include_graphics(paste0("figures/M5_trace_3.png"))

```

:::{custom-style="Image Caption"}
`r fig.trace.3.num` Trace plots of parameters with high `r R_hat` values.
:::

### Pareto-k statistics {.unnumbered}

```{r plot-pareto-k, echo=FALSE, message=FALSE}
fig.pareto.num <- run_autonum(seq_id = "fig", 
                              pre_label = "Figure A",
                              bkm = "Figure-pareto-k")

knitr::include_graphics(paste0("figures/High_Pareto_k.png"))


```

:::{custom-style="Image Caption"}
`r fig.pareto.num` Good (< 0.7) and bad (>= 0.7) Pareto-k statistics for each data point. y-axis indicates the observed number of gray whales per watch period.
:::

### Changes in S1 and S2 over time {.unnumbered}

```{r plot-S1-posteriors, echo=FALSE, message=FALSE}

fig.S1.num <- run_autonum(seq_id = "fig",
                          pre_label = "Figure A",
                          bkm = "Figure-S1-posteriors")

knitr::include_graphics(paste0("figures/posteriors_S1_", dpi.set, "dpi.png"))

```

:::{custom-style="Image Caption"}
`r fig.S1.num` Changes in the $S_1$ parameter of the Richards equation over time. $S_1$ defines the width of the right side of the curve.
:::

```{r plot-S2-posteriors, echo=FALSE, message=FALSE}
fig.S2.num <- run_autonum(seq_id = "fig",
                          pre_label = "Figure A",
                          bkm = "Figure-S2-posteriors")

knitr::include_graphics(paste0("figures/posteriors_S2_", dpi.set, "dpi.png"))

```

:::{custom-style="Image Caption"}
`r fig.S2.num` Changes in the $S_2$ parameter of the Richards equation over time. $S_2$ defines the width of the left side of the curve.
:::

### Observer random effects {.unnumbered}

```{r plot-observers, echo=FALSE, message=FALSE}
fig.OBS.num <- run_autonum(seq_id = "fig",
                           pre_label = "Figure A",
                           bkm = "Figure-observers")

knitr::include_graphics(paste0("figures/Observer_dens.png"))

```

:::{custom-style="Image Caption"}
`r fig.OBS.num` Marginal posterior density distributions of observer random effects. Observers with less than 10 sightings were pooled as one observer.
:::

## Literature cited {.unnumbered}

Goodrich B, Gabry J, Ali I & Brilleman S. (2024). rstanarm: Bayesian applied regression modeling via Stan. R package version 2.32.1 <https://mc-stan.org/rstanarm>.

Kellner K (2024). jagsUI: A Wrapper Around 'rjags' to Streamline 'JAGS' Analyses\_. R package version 1.6.2, <https://CRAN.R-project.org/package=jagsUI>.

Vehtari A, Gabry J, Magnusson M, Yao Y, Bürkner P, Paananen T, Gelman A (2024). “loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models.” R package version 2.8.0, <https://mc-stan.org/loo/>.
